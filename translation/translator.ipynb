{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from google.transliteration import transliterate_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hatexplain (C:/Users/dange/.cache/huggingface/datasets/hatexplain/plain_text/1.0.0/df474d8d8667d89ef30649bf66e9c856ad8305bef4bc147e8e31cbdf1b8e0249)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c40a6ea25b48b58ca7f849ccb9a907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"hatexplain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(dataset, split, max_len=200):\n",
    "    dataset = dataset[split].to_dict()\n",
    "    del dataset['id']\n",
    "    num_examples = len(dataset['post_tokens'])\n",
    "    print(f'{split} has {num_examples} examples')\n",
    "    dataset['label'] = torch.zeros((num_examples, 3))\n",
    "    label = []\n",
    "    for i in range(num_examples):\n",
    "        label.append(torch.Tensor(torch.Tensor(dataset['annotators'][i]['label']).type(torch.IntTensor)))\n",
    "    label = torch.stack(label)\n",
    "    label = label.mode().values\n",
    "    dataset['label'][torch.arange(num_examples).type(torch.LongTensor), label.type(torch.LongTensor)] = 1\n",
    "    dataset['class'] = label\n",
    "    rationales = []\n",
    "    for rationale in dataset['rationales']:\n",
    "        if len(rationale) == 0:\n",
    "            rationales.append(torch.zeros((max_len)))\n",
    "            continue\n",
    "        r = np.concatenate((\n",
    "            np.array(rationale[0]), np.zeros((max_len - len(rationale[0])))\n",
    "        )).astype(bool)\n",
    "        for i in range(1, len(rationale)):\n",
    "            r += np.concatenate((\n",
    "                np.array(rationale[i]), np.zeros((max_len - len(rationale[i])))\n",
    "            )).astype(bool)\n",
    "        rationales.append(torch.tensor((r).astype(int)))\n",
    "    dataset['rationales'] = torch.stack(rationales)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 15383 examples\n",
      "validation has 1922 examples\n",
      "test has 1924 examples\n"
     ]
    }
   ],
   "source": [
    "train = create_dataframe(dataset, 'train')\n",
    "validation = create_dataframe(dataset, 'validation')\n",
    "test = create_dataframe(dataset, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train class split:-  tensor([4748, 6251, 4384]) \n",
      " validation class split:-  tensor([593, 781, 548]) \n",
      " test class split:-  tensor([594, 782, 548])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"train class split:- \", train['class'].bincount(), \"\\n\",\n",
    "    \"validation class split:- \", validation['class'].bincount(), \"\\n\",\n",
    "    \"test class split:- \", test['class'].bincount(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX LENGTH:- 165\n",
      "AVG LENGTH:- 23.465253851654424\n"
     ]
    }
   ],
   "source": [
    "sent_len = []\n",
    "for sent in train['post_tokens']:\n",
    "    sent_len.append(len(sent))\n",
    "print(f'MAX LENGTH:- {max(sent_len)}\\nAVG LENGTH:- {sum(sent_len)/len(sent_len)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "stratified_shuffle = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_language_splits(dataset, shuffler):\n",
    "    shuffler.get_n_splits(dataset['post_tokens'], dataset['class'])\n",
    "    for en_index, hi_index in shuffler.split(dataset['post_tokens'], dataset['class']):\n",
    "        en_dataset = {}\n",
    "        hi_dataset = {}\n",
    "\n",
    "        en_dataset['index'] = en_index\n",
    "        hi_dataset['index'] = hi_index\n",
    "\n",
    "        en_dataset['label'] = torch.index_select(dataset['label'], 0, torch.tensor(en_index))\n",
    "        hi_dataset['label'] = torch.index_select(dataset['label'], 0, torch.tensor(hi_index))\n",
    "\n",
    "        en_dataset['rationales'] = torch.index_select(dataset['rationales'], 0, torch.tensor(en_index))\n",
    "        hi_dataset['rationales'] = torch.index_select(dataset['rationales'], 0, torch.tensor(hi_index))\n",
    "\n",
    "        en_dataset['class'] = torch.tensor([dataset['class'][i] for i in en_index])\n",
    "        hi_dataset['class'] = torch.tensor([dataset['class'][i] for i in hi_index])\n",
    "\n",
    "        en_dataset['post_tokens'] = [dataset['post_tokens'][i] for i in en_index]\n",
    "        hi_dataset['post_tokens'] = [dataset['post_tokens'][i] for i in hi_index]\n",
    "\n",
    "    return en_dataset, hi_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train, hi_train = generate_language_splits(train, stratified_shuffle)\n",
    "en_validation, hi_validation = generate_language_splits(validation, stratified_shuffle)\n",
    "en_test, hi_test = generate_language_splits(test, stratified_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train class split:-  tensor([ 950, 1250,  877]) \n",
      " validation class split:-  tensor([119, 156, 110]) \n",
      " test class split:-  tensor([119, 156, 110])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"train class split:- \", hi_train['class'].bincount(), \"\\n\",\n",
    "    \"validation class split:- \", hi_validation['class'].bincount(), \"\\n\",\n",
    "    \"test class split:- \", hi_test['class'].bincount(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "transliterator = {}\n",
    "transliterator['sentence'] = []\n",
    "transliterator['index'] = []\n",
    "transliterator['words'] = []\n",
    "transliterator['type'] = []\n",
    "for i in range(hi_train['class'].shape[0]):\n",
    "    transliterator['type'].append('train')\n",
    "    transliterator['sentence'].append(\" \".join(hi_train['post_tokens'][i]))\n",
    "    transliterator['index'].append(hi_train['index'][i])\n",
    "    mask = hi_train['rationales'][i, :]\n",
    "    transliterator['words'].append(\n",
    "        \" \".join([word for i, word in enumerate(hi_train['post_tokens'][i]) if i < 200 and mask[i]])\n",
    "    )\n",
    "for i in range(hi_validation['class'].shape[0]):\n",
    "    transliterator['type'].append('validation')\n",
    "    transliterator['sentence'].append(\" \".join(hi_validation['post_tokens'][i]))\n",
    "    transliterator['index'].append(hi_validation['index'][i])\n",
    "    mask = hi_validation['rationales'][i, :]\n",
    "    transliterator['words'].append(\n",
    "        \" \".join([word for i, word in enumerate(hi_validation['post_tokens'][i]) if i < 200 and mask[i]])\n",
    "    )\n",
    "for i in range(hi_test['class'].shape[0]):\n",
    "    transliterator['type'].append('test')\n",
    "    transliterator['sentence'].append(\" \".join(hi_test['post_tokens'][i]))\n",
    "    transliterator['index'].append(hi_test['index'][i])\n",
    "    mask = hi_test['rationales'][i, :]\n",
    "    transliterator['words'].append(\n",
    "        \" \".join([word for i, word in enumerate(hi_test['post_tokens'][i]) if i < 200 and mask[i]])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(transliterator)\n",
    "df.to_csv('to_translate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_df = pd.read_csv('../translation/translated_20.csv', usecols=['sentence', 'hindi_sentence', 'words', 'hindi_words', 'index', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>index</th>\n",
       "      <th>words</th>\n",
       "      <th>type</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>hindi_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>luckily this kike has never lived under nazi r...</td>\n",
       "      <td>740</td>\n",
       "      <td>kike has never lived under nazi kikes sand nig...</td>\n",
       "      <td>train</td>\n",
       "      <td>सौभाग्य से यह Kike नाजी शासन के तहत कभी नहीं र...</td>\n",
       "      <td>Kike कभी भी नाजी किक सैंड निगर्स के तहत नहीं रहा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ohh man that is a fucking sad photo lots of re...</td>\n",
       "      <td>12989</td>\n",
       "      <td>sand nigger countries let the moslems kill eac...</td>\n",
       "      <td>train</td>\n",
       "      <td>ओह यार जो कि सैनिकों के लिए एक कमबख्त दुखद फोट...</td>\n",
       "      <td>रेत निगर देशों ने मोस्ले को एक दूसरे को मारने ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>miss lindsey is john mccains boy toy miss lind...</td>\n",
       "      <td>12636</td>\n",
       "      <td>demented faggot</td>\n",
       "      <td>train</td>\n",
       "      <td>मिस लिंडसे जॉन मैककेन्स बॉय टॉय मिस लिंडसे एक ...</td>\n",
       "      <td>डिम्फ़ फागोट</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asians dont deserve happiness</td>\n",
       "      <td>12054</td>\n",
       "      <td>asians dont deserve happiness</td>\n",
       "      <td>train</td>\n",
       "      <td>एशियाई लोग खुशी के लायक नहीं हैं</td>\n",
       "      <td>एशियाई लोग खुशी के लायक नहीं हैं</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; sikh community always helpful to others...</td>\n",
       "      <td>15220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;user&gt; सिख समुदाय हमेशा दूसरों के लिए सहायक रू...</td>\n",
       "      <td>#VALUE!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3842</th>\n",
       "      <td>&lt;user&gt; hari raya is not about people going fro...</td>\n",
       "      <td>1134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;उपयोगकर्ता&gt; हरि राया घरों से घरों तक जाने वाल...</td>\n",
       "      <td>#VALUE!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>no stop pointing the finger at men shame women...</td>\n",
       "      <td>1412</td>\n",
       "      <td>stop pointing the finger at men shame women wh...</td>\n",
       "      <td>test</td>\n",
       "      <td>पुरुषों पर उंगली की ओर इशारा करने वाला कोई भी ...</td>\n",
       "      <td>पुरुषों की उंगली की ओर इशारा करना बंद करो महिल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3844</th>\n",
       "      <td>america is so racist that they think asians ar...</td>\n",
       "      <td>386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>अमेरिका इतना नस्लवादी है कि उन्हें लगता है कि ...</td>\n",
       "      <td>#VALUE!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>this sperging alt right retard accused me of b...</td>\n",
       "      <td>293</td>\n",
       "      <td>this sperging alt right retard accused me of b...</td>\n",
       "      <td>test</td>\n",
       "      <td>इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...</td>\n",
       "      <td>इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>you promote articles by fat dyke pipe fitters ...</td>\n",
       "      <td>1100</td>\n",
       "      <td>by fat dyke pipe fitters</td>\n",
       "      <td>test</td>\n",
       "      <td>आप फैट डाइक पाइप फिटर द्वारा लेखों को बढ़ावा द...</td>\n",
       "      <td>#VALUE!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3847 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  index  \\\n",
       "0     luckily this kike has never lived under nazi r...    740   \n",
       "1     ohh man that is a fucking sad photo lots of re...  12989   \n",
       "2     miss lindsey is john mccains boy toy miss lind...  12636   \n",
       "3                         asians dont deserve happiness  12054   \n",
       "4     <user> sikh community always helpful to others...  15220   \n",
       "...                                                 ...    ...   \n",
       "3842  <user> hari raya is not about people going fro...   1134   \n",
       "3843  no stop pointing the finger at men shame women...   1412   \n",
       "3844  america is so racist that they think asians ar...    386   \n",
       "3845  this sperging alt right retard accused me of b...    293   \n",
       "3846  you promote articles by fat dyke pipe fitters ...   1100   \n",
       "\n",
       "                                                  words   type  \\\n",
       "0     kike has never lived under nazi kikes sand nig...  train   \n",
       "1     sand nigger countries let the moslems kill eac...  train   \n",
       "2                                       demented faggot  train   \n",
       "3                         asians dont deserve happiness  train   \n",
       "4                                                   NaN  train   \n",
       "...                                                 ...    ...   \n",
       "3842                                                NaN   test   \n",
       "3843  stop pointing the finger at men shame women wh...   test   \n",
       "3844                                                NaN   test   \n",
       "3845  this sperging alt right retard accused me of b...   test   \n",
       "3846                           by fat dyke pipe fitters   test   \n",
       "\n",
       "                                         hindi_sentence  \\\n",
       "0     सौभाग्य से यह Kike नाजी शासन के तहत कभी नहीं र...   \n",
       "1     ओह यार जो कि सैनिकों के लिए एक कमबख्त दुखद फोट...   \n",
       "2     मिस लिंडसे जॉन मैककेन्स बॉय टॉय मिस लिंडसे एक ...   \n",
       "3                      एशियाई लोग खुशी के लायक नहीं हैं   \n",
       "4     <user> सिख समुदाय हमेशा दूसरों के लिए सहायक रू...   \n",
       "...                                                 ...   \n",
       "3842  <उपयोगकर्ता> हरि राया घरों से घरों तक जाने वाल...   \n",
       "3843  पुरुषों पर उंगली की ओर इशारा करने वाला कोई भी ...   \n",
       "3844  अमेरिका इतना नस्लवादी है कि उन्हें लगता है कि ...   \n",
       "3845  इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...   \n",
       "3846  आप फैट डाइक पाइप फिटर द्वारा लेखों को बढ़ावा द...   \n",
       "\n",
       "                                            hindi_words  \n",
       "0      Kike कभी भी नाजी किक सैंड निगर्स के तहत नहीं रहा  \n",
       "1     रेत निगर देशों ने मोस्ले को एक दूसरे को मारने ...  \n",
       "2                                          डिम्फ़ फागोट  \n",
       "3                      एशियाई लोग खुशी के लायक नहीं हैं  \n",
       "4                                               #VALUE!  \n",
       "...                                                 ...  \n",
       "3842                                            #VALUE!  \n",
       "3843  पुरुषों की उंगली की ओर इशारा करना बंद करो महिल...  \n",
       "3844                                            #VALUE!  \n",
       "3845  इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...  \n",
       "3846                                            #VALUE!  \n",
       "\n",
       "[3847 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(hindi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_alphabets = [\n",
    "    \"क\",\"ख\",\"ग\",\"घ\",\"ङ\",\"च\",\"छ\",\"ज\",\"झ\",\"ञ\",\"ट\",\"ठ\",\"ड\",\"ढ\",\"ण\",\"त\",\"थ\",\"द\",\"ध\",\"न\",\"प\",\"फ\",\"ब\",\"भ\",\"म\",\"य\",\"र\",\"ल\",\"व\",\"श\",\"ष\",\"स\",\"ह\",\"क्ष\",\"त्र\",\"ज्ञ\"    \n",
    "    ]\n",
    "def normalize(input):\n",
    "    input_type = type(input)\n",
    "    if input_type == str:\n",
    "        input = input.split()\n",
    "    output = []\n",
    "    for word in input:\n",
    "        norm = []\n",
    "        for char in list(word):\n",
    "            if char in hindi_alphabets:\n",
    "                norm.append(char)\n",
    "        output.append(\"\".join(norm))\n",
    "    if input_type == str:\n",
    "        return \" \".join(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentence', 'index', 'words', 'type', 'hindi_sentence', 'hindi_words'])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_dict = hindi_df.to_dict()\n",
    "translated_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_sentences = hindi_df.hindi_sentence.values\n",
    "hindi_words = hindi_df.hindi_words.values\n",
    "english_words = hindi_df.words.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ae5eb19116432aadf769f21b298899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3847 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23065 words mapped out of 100875\n",
      "3681 rationales found out of 3847\n"
     ]
    }
   ],
   "source": [
    "hindi_rationales = []\n",
    "\n",
    "total_words = 0\n",
    "matches_found = 0\n",
    "sentences_matched = 0\n",
    "MATCHED = False\n",
    "for i, sentence in enumerate(tqdm(hindi_sentences)):\n",
    "    sentence = sentence.split()\n",
    "    rationale = torch.zeros(1, len(sentence))\n",
    "    total_words += len(hindi_words[i])\n",
    "    if hindi_words[i] == '#VALUE!':\n",
    "        hindi_rationales.append(rationale)\n",
    "        sentences_matched += 1\n",
    "        continue\n",
    "    transliterated_words = []\n",
    "    if type(hindi_words[i]) != list: \n",
    "        hindi_words[i] = hindi_words[i].split() \n",
    "    if type(english_words[i]) != list: \n",
    "        english_words[i] = english_words[i].split()\n",
    "        for word in english_words[i]:\n",
    "            transliterated_words.extend(transliterate_word(word, lang_code='hi'))\n",
    "    normalized_hindi_words = normalize(hindi_words[i])\n",
    "    normalized_trans_words = normalize(transliterated_words)\n",
    "    for j, word in enumerate(sentence):\n",
    "        if word in hindi_words[i] or word in english_words[i] or word in transliterated_words:\n",
    "            matches_found += 1\n",
    "            rationale[0,j] = 1\n",
    "            if not MATCHED:\n",
    "                sentences_matched += 1\n",
    "                MATCHED = True\n",
    "        elif normalize(word) in normalized_hindi_words or normalize(word) in normalized_trans_words:\n",
    "            matches_found += 1\n",
    "            rationale[0, j] = 1\n",
    "            if not MATCHED:\n",
    "                sentences_matched += 1\n",
    "                MATCHED = True\n",
    "    if not MATCHED:\n",
    "        hindi_rationales.append(None)\n",
    "    else:\n",
    "        hindi_rationales.append(rationale)\n",
    "    MATCHED = False\n",
    "print(f'{matches_found} words mapped out of {total_words}\\n{sentences_matched} rationales found out of {hindi_df.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dange\\.conda\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:576: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "c:\\Users\\dange\\.conda\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:576: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>index</th>\n",
       "      <th>words</th>\n",
       "      <th>type</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>hindi_words</th>\n",
       "      <th>hindi_rationales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>luckily this kike has never lived under nazi r...</td>\n",
       "      <td>740</td>\n",
       "      <td>kike has never lived under nazi kikes sand nig...</td>\n",
       "      <td>train</td>\n",
       "      <td>सौभाग्य से यह Kike नाजी शासन के तहत कभी नहीं र...</td>\n",
       "      <td>Kike कभी भी नाजी किक सैंड निगर्स के तहत नहीं रहा</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ohh man that is a fucking sad photo lots of re...</td>\n",
       "      <td>12989</td>\n",
       "      <td>sand nigger countries let the moslems kill eac...</td>\n",
       "      <td>train</td>\n",
       "      <td>ओह यार जो कि सैनिकों के लिए एक कमबख्त दुखद फोट...</td>\n",
       "      <td>रेत निगर देशों ने मोस्ले को एक दूसरे को मारने ...</td>\n",
       "      <td>[[tensor(1.), tensor(0.), tensor(1.), tensor(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>miss lindsey is john mccains boy toy miss lind...</td>\n",
       "      <td>12636</td>\n",
       "      <td>demented faggot</td>\n",
       "      <td>train</td>\n",
       "      <td>मिस लिंडसे जॉन मैककेन्स बॉय टॉय मिस लिंडसे एक ...</td>\n",
       "      <td>डिम्फ़ फागोट</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asians dont deserve happiness</td>\n",
       "      <td>12054</td>\n",
       "      <td>asians dont deserve happiness</td>\n",
       "      <td>train</td>\n",
       "      <td>एशियाई लोग खुशी के लायक नहीं हैं</td>\n",
       "      <td>एशियाई लोग खुशी के लायक नहीं हैं</td>\n",
       "      <td>[[tensor(1.), tensor(1.), tensor(1.), tensor(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; sikh community always helpful to others...</td>\n",
       "      <td>15220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;user&gt; सिख समुदाय हमेशा दूसरों के लिए सहायक रू...</td>\n",
       "      <td>#VALUE!</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3842</th>\n",
       "      <td>&lt;user&gt; hari raya is not about people going fro...</td>\n",
       "      <td>1134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;उपयोगकर्ता&gt; हरि राया घरों से घरों तक जाने वाल...</td>\n",
       "      <td>#VALUE!</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>no stop pointing the finger at men shame women...</td>\n",
       "      <td>1412</td>\n",
       "      <td>stop pointing the finger at men shame women wh...</td>\n",
       "      <td>test</td>\n",
       "      <td>पुरुषों पर उंगली की ओर इशारा करने वाला कोई भी ...</td>\n",
       "      <td>पुरुषों की उंगली की ओर इशारा करना बंद करो महिल...</td>\n",
       "      <td>[[tensor(1.), tensor(0.), tensor(1.), tensor(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3844</th>\n",
       "      <td>america is so racist that they think asians ar...</td>\n",
       "      <td>386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>अमेरिका इतना नस्लवादी है कि उन्हें लगता है कि ...</td>\n",
       "      <td>#VALUE!</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>this sperging alt right retard accused me of b...</td>\n",
       "      <td>293</td>\n",
       "      <td>this sperging alt right retard accused me of b...</td>\n",
       "      <td>test</td>\n",
       "      <td>इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...</td>\n",
       "      <td>इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...</td>\n",
       "      <td>[[tensor(1.), tensor(1.), tensor(1.), tensor(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>you promote articles by fat dyke pipe fitters ...</td>\n",
       "      <td>1100</td>\n",
       "      <td>by fat dyke pipe fitters</td>\n",
       "      <td>test</td>\n",
       "      <td>आप फैट डाइक पाइप फिटर द्वारा लेखों को बढ़ावा द...</td>\n",
       "      <td>#VALUE!</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3847 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  index  \\\n",
       "0     luckily this kike has never lived under nazi r...    740   \n",
       "1     ohh man that is a fucking sad photo lots of re...  12989   \n",
       "2     miss lindsey is john mccains boy toy miss lind...  12636   \n",
       "3                         asians dont deserve happiness  12054   \n",
       "4     <user> sikh community always helpful to others...  15220   \n",
       "...                                                 ...    ...   \n",
       "3842  <user> hari raya is not about people going fro...   1134   \n",
       "3843  no stop pointing the finger at men shame women...   1412   \n",
       "3844  america is so racist that they think asians ar...    386   \n",
       "3845  this sperging alt right retard accused me of b...    293   \n",
       "3846  you promote articles by fat dyke pipe fitters ...   1100   \n",
       "\n",
       "                                                  words   type  \\\n",
       "0     kike has never lived under nazi kikes sand nig...  train   \n",
       "1     sand nigger countries let the moslems kill eac...  train   \n",
       "2                                       demented faggot  train   \n",
       "3                         asians dont deserve happiness  train   \n",
       "4                                                   NaN  train   \n",
       "...                                                 ...    ...   \n",
       "3842                                                NaN   test   \n",
       "3843  stop pointing the finger at men shame women wh...   test   \n",
       "3844                                                NaN   test   \n",
       "3845  this sperging alt right retard accused me of b...   test   \n",
       "3846                           by fat dyke pipe fitters   test   \n",
       "\n",
       "                                         hindi_sentence  \\\n",
       "0     सौभाग्य से यह Kike नाजी शासन के तहत कभी नहीं र...   \n",
       "1     ओह यार जो कि सैनिकों के लिए एक कमबख्त दुखद फोट...   \n",
       "2     मिस लिंडसे जॉन मैककेन्स बॉय टॉय मिस लिंडसे एक ...   \n",
       "3                      एशियाई लोग खुशी के लायक नहीं हैं   \n",
       "4     <user> सिख समुदाय हमेशा दूसरों के लिए सहायक रू...   \n",
       "...                                                 ...   \n",
       "3842  <उपयोगकर्ता> हरि राया घरों से घरों तक जाने वाल...   \n",
       "3843  पुरुषों पर उंगली की ओर इशारा करने वाला कोई भी ...   \n",
       "3844  अमेरिका इतना नस्लवादी है कि उन्हें लगता है कि ...   \n",
       "3845  इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...   \n",
       "3846  आप फैट डाइक पाइप फिटर द्वारा लेखों को बढ़ावा द...   \n",
       "\n",
       "                                            hindi_words  \\\n",
       "0      Kike कभी भी नाजी किक सैंड निगर्स के तहत नहीं रहा   \n",
       "1     रेत निगर देशों ने मोस्ले को एक दूसरे को मारने ...   \n",
       "2                                          डिम्फ़ फागोट   \n",
       "3                      एशियाई लोग खुशी के लायक नहीं हैं   \n",
       "4                                               #VALUE!   \n",
       "...                                                 ...   \n",
       "3842                                            #VALUE!   \n",
       "3843  पुरुषों की उंगली की ओर इशारा करना बंद करो महिल...   \n",
       "3844                                            #VALUE!   \n",
       "3845  इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...   \n",
       "3846                                            #VALUE!   \n",
       "\n",
       "                                       hindi_rationales  \n",
       "0     [[tensor(0.), tensor(0.), tensor(0.), tensor(1...  \n",
       "1     [[tensor(1.), tensor(0.), tensor(1.), tensor(1...  \n",
       "2     [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  \n",
       "3     [[tensor(1.), tensor(1.), tensor(1.), tensor(1...  \n",
       "4     [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  \n",
       "...                                                 ...  \n",
       "3842  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  \n",
       "3843  [[tensor(1.), tensor(0.), tensor(1.), tensor(1...  \n",
       "3844  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  \n",
       "3845  [[tensor(1.), tensor(1.), tensor(1.), tensor(1...  \n",
       "3846  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  \n",
       "\n",
       "[3847 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'hindi_rationales' in hindi_df.columns:\n",
    "    print(\"cleaning . . . \")\n",
    "    hindi_df.drop(columns=['hindi_rationales'])\n",
    "    hindi_df.dropna()\n",
    "hindi_df = pd.concat([hindi_df, pd.DataFrame(hindi_rationales, columns=['hindi_rationales'])], axis=1)\n",
    "display(hindi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_english(en_dict, index, label, rationale, output, post_tokens):\n",
    "    en_dict['index'] = np.concatenate((en_dict['index'], np.array([index])))\n",
    "    en_dict['label'] = np.concatenate((en_dict['label'], label.reshape(1,3)))\n",
    "    en_dict['rationales'] = np.concatenate((en_dict['rationales'], rationale.reshape(1, 200)))\n",
    "    en_dict['class'] = np.concatenate((en_dict['class'], np.array([output])))\n",
    "    en_dict['post_tokens'].append(post_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'label', 'rationales', 'class', 'post_tokens'])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final(translated_dict, en_dict, hi_dict, hindi_df):\n",
    "    for i, row in enumerate(hindi_df.iterrows()):\n",
    "        _, row = row\n",
    "        if row[-1] == None:\n",
    "            append_to_english(en_dict, row[1], hi_dict['label'][i], hi_dict['rationales'][i], hi_dict['class'][i], hi_dict['post_tokens'][i])\n",
    "        else:\n",
    "            translated_dict['index'].append(row[1])\n",
    "            translated_dict['label'].append(hi_dict['label'][i])\n",
    "            translated_dict['rationales'].append(row[-1])\n",
    "            translated_dict['class'].append(hi_dict['class'][i])\n",
    "            translated_dict['post_tokens'].append(row[4].split(\" \"))\n",
    "    translated_dict['label'] = torch.stack(translated_dict['label'])\n",
    "    translated_dict['class'] = torch.stack(translated_dict['class'])\n",
    "    rationales = []\n",
    "    for rationale in translated_dict['rationales']:\n",
    "        r = np.concatenate(\n",
    "            (rationale, \n",
    "                np.zeros((1, 200 - rationale.shape[1]))\n",
    "            ), axis=1).astype(bool)\n",
    "        rationales.append(torch.tensor((r).astype(int)))\n",
    "    translated_dict['rationales'] = torch.stack(rationales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_hi_train = {}\n",
    "translated_hi_validation = {}\n",
    "translated_hi_test = {}\n",
    "\n",
    "for key in en_train.keys():\n",
    "    translated_hi_train[key] = []\n",
    "    translated_hi_validation[key] = []\n",
    "    translated_hi_test[key] = []\n",
    "\n",
    "groups = hindi_df.groupby('type')\n",
    "generate_final(translated_hi_train, en_train, hi_train, groups.get_group('train'))\n",
    "generate_final(translated_hi_validation, en_validation, hi_validation, groups.get_group('validation'))\n",
    "generate_final(translated_hi_test, en_test, hi_test, groups.get_group('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train class split:-  tensor([ 879, 1250,  816]) \n",
      " validation class split:-  tensor([114, 156, 104]) \n",
      " test class split:-  tensor([110, 156,  96])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"train class split:- \", translated_hi_train['class'].bincount(), \"\\n\",\n",
    "    \"validation class split:- \", translated_hi_validation['class'].bincount(), \"\\n\",\n",
    "    \"test class split:- \", translated_hi_test['class'].bincount(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train class split:-  tensor([3869, 5001, 3568]) \n",
      " validation class split:-  tensor([479, 625, 444]) \n",
      " test class split:-  tensor([484, 626, 452])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"train class split:- \", torch.tensor(en_train['class']).bincount(), \"\\n\",\n",
    "    \"validation class split:- \", torch.tensor(en_validation['class']).bincount(), \"\\n\",\n",
    "    \"test class split:- \", torch.tensor(en_test['class']).bincount(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4748, 6251, 4384]) tensor([593, 781, 548]) tensor([594, 782, 548])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    torch.tensor(en_train['class']).bincount() + translated_hi_train['class'].bincount(), \n",
    "torch.tensor(en_validation['class']).bincount() + translated_hi_validation['class'].bincount(), \n",
    "torch.tensor(en_test['class']).bincount() + translated_hi_test['class'].bincount()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train class split:-  tensor([4748, 6251, 4384]) \n",
      " validation class split:-  tensor([593, 781, 548]) \n",
      " test class split:-  tensor([594, 782, 548])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"train class split:- \", train['class'].bincount(), \"\\n\",\n",
    "    \"validation class split:- \", validation['class'].bincount(), \"\\n\",\n",
    "    \"test class split:- \", test['class'].bincount(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(en_train, open('../data/en_train.p', 'wb'))\n",
    "pickle.dump(en_validation, open('../data/en_validation.p', 'wb'))\n",
    "pickle.dump(en_test, open('../data/en_test.p', 'wb'))\n",
    "\n",
    "pickle.dump(translated_hi_train, open('../data/hi_train.p', 'wb'))\n",
    "pickle.dump(hi_validation, open('../data/hi_validation.p', 'wb'))\n",
    "pickle.dump(hi_test, open('../data/hi_test.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c165b169ead51ed7dd867ada967038e7afce51eef97009d1ebd4bca797cfdb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
