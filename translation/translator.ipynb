{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from google.transliteration import transliterate_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hatexplain (C:/Users/dange/.cache/huggingface/datasets/hatexplain/plain_text/1.0.0/df474d8d8667d89ef30649bf66e9c856ad8305bef4bc147e8e31cbdf1b8e0249)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c40a6ea25b48b58ca7f849ccb9a907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"hatexplain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(dataset, split, max_len=200):\n",
    "    dataset = dataset[split].to_dict()\n",
    "    del dataset['id']\n",
    "    num_examples = len(dataset['post_tokens'])\n",
    "    print(f'{split} has {num_examples} examples')\n",
    "    dataset['label'] = torch.zeros((num_examples, 3))\n",
    "    label = []\n",
    "    for i in range(num_examples):\n",
    "        label.append(torch.Tensor(torch.Tensor(dataset['annotators'][i]['label']).type(torch.IntTensor)))\n",
    "    label = torch.stack(label)\n",
    "    label = label.mode().values\n",
    "    dataset['label'][torch.arange(num_examples).type(torch.LongTensor), label.type(torch.LongTensor)] = 1\n",
    "    dataset['class'] = label\n",
    "    rationales = []\n",
    "    for rationale in dataset['rationales']:\n",
    "        if len(rationale) == 0:\n",
    "            rationales.append(torch.zeros((max_len)))\n",
    "            continue\n",
    "        r = np.concatenate((\n",
    "            np.array(rationale[0]), np.zeros((max_len - len(rationale[0])))\n",
    "        )).astype(bool)\n",
    "        for i in range(1, len(rationale)):\n",
    "            r += np.concatenate((\n",
    "                np.array(rationale[i]), np.zeros((max_len - len(rationale[i])))\n",
    "            )).astype(bool)\n",
    "        rationales.append(torch.tensor((r).astype(int)))\n",
    "    dataset['rationales'] = torch.stack(rationales)\n",
    "    sentences = []\n",
    "    for sentence in dataset['post_tokens']:\n",
    "        sentences.append(\" \".join(sentence))\n",
    "    dataset['post_tokens'] = sentences\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 15383 examples\n",
      "validation has 1922 examples\n",
      "test has 1924 examples\n"
     ]
    }
   ],
   "source": [
    "train = create_dataframe(dataset, 'train')\n",
    "validation = create_dataframe(dataset, 'validation')\n",
    "test = create_dataframe(dataset, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train class split:-  tensor([4748, 6251, 4384]) \n",
      " validation class split:-  tensor([593, 781, 548]) \n",
      " test class split:-  tensor([594, 782, 548])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"train class split:- \", train['class'].bincount(), \"\\n\",\n",
    "    \"validation class split:- \", validation['class'].bincount(), \"\\n\",\n",
    "    \"test class split:- \", test['class'].bincount(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX LENGTH:- 526\n",
      "AVG LENGTH:- 126.54527725411168\n"
     ]
    }
   ],
   "source": [
    "sent_len = []\n",
    "for sent in train['post_tokens']:\n",
    "    sent_len.append(len(sent))\n",
    "print(f'MAX LENGTH:- {max(sent_len)}\\nAVG LENGTH:- {sum(sent_len)/len(sent_len)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "stratified_shuffle = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_language_splits(dataset, shuffler):\n",
    "    shuffler.get_n_splits(dataset['post_tokens'], dataset['class'])\n",
    "    for en_index, hi_index in shuffler.split(dataset['post_tokens'], dataset['class']):\n",
    "        en_dataset = {}\n",
    "        hi_dataset = {}\n",
    "\n",
    "        en_dataset['index'] = en_index\n",
    "        hi_dataset['index'] = hi_index\n",
    "\n",
    "        en_dataset['label'] = torch.index_select(dataset['label'], 0, torch.tensor(en_index))\n",
    "        hi_dataset['label'] = torch.index_select(dataset['label'], 0, torch.tensor(hi_index))\n",
    "\n",
    "        en_dataset['rationales'] = torch.index_select(dataset['rationales'], 0, torch.tensor(en_index))\n",
    "        hi_dataset['rationales'] = torch.index_select(dataset['rationales'], 0, torch.tensor(hi_index))\n",
    "\n",
    "        en_dataset['class'] = torch.tensor([dataset['class'][i] for i in en_index])\n",
    "        hi_dataset['class'] = torch.tensor([dataset['class'][i] for i in hi_index])\n",
    "\n",
    "        en_dataset['post_tokens'] = [dataset['post_tokens'][i] for i in en_index]\n",
    "        hi_dataset['post_tokens'] = [dataset['post_tokens'][i] for i in hi_index]\n",
    "\n",
    "    return en_dataset, hi_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train, hi_train = generate_language_splits(train, stratified_shuffle)\n",
    "en_validation, hi_validation = generate_language_splits(validation, stratified_shuffle)\n",
    "en_test, hi_test = generate_language_splits(test, stratified_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train class split:-  tensor([ 950, 1250,  877]) \n",
      " validation class split:-  tensor([119, 156, 110]) \n",
      " test class split:-  tensor([119, 156, 110])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"train class split:- \", hi_train['class'].bincount(), \"\\n\",\n",
    "    \"validation class split:- \", hi_validation['class'].bincount(), \"\\n\",\n",
    "    \"test class split:- \", hi_test['class'].bincount(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "transliterator = {}\n",
    "transliterator['sentence'] = []\n",
    "transliterator['index'] = []\n",
    "transliterator['words'] = []\n",
    "transliterator['type'] = []\n",
    "for i in range(hi_train['class'].shape[0]):\n",
    "    transliterator['type'].append('train')\n",
    "    transliterator['sentence'].append(hi_train['post_tokens'][i])\n",
    "    transliterator['index'].append(hi_train['index'][i])\n",
    "    mask = hi_train['rationales'][i, :]\n",
    "    transliterator['words'].append(\n",
    "        \" \".join([word for i, word in enumerate(hi_train['post_tokens'][i]) if i < 200 and mask[i]])\n",
    "    )\n",
    "for i in range(hi_validation['class'].shape[0]):\n",
    "    transliterator['type'].append('validation')\n",
    "    transliterator['sentence'].append(hi_train['post_tokens'][i])\n",
    "    transliterator['index'].append(hi_validation['index'][i])\n",
    "    mask = hi_validation['rationales'][i, :]\n",
    "    transliterator['words'].append(\n",
    "        \" \".join([word for i, word in enumerate(hi_validation['post_tokens'][i]) if i < 200 and mask[i]])\n",
    "    )\n",
    "for i in range(hi_test['class'].shape[0]):\n",
    "    transliterator['type'].append('test')\n",
    "    transliterator['sentence'].append(hi_train['post_tokens'][i])\n",
    "    transliterator['index'].append(hi_test['index'][i])\n",
    "    mask = hi_test['rationales'][i, :]\n",
    "    transliterator['words'].append(\n",
    "        \" \".join([word for i, word in enumerate(hi_test['post_tokens'][i]) if i < 200 and mask[i]])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(transliterator)\n",
    "df.to_csv('to_translate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../translation/translated_20.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dange\\Documents\\roles\\engineer\\nlp\\cs-626-project-hate-speech-detection\\translation\\translator.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dange/Documents/roles/engineer/nlp/cs-626-project-hate-speech-detection/translation/translator.ipynb#Y126sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m hindi_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m../translation/translated_20.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, usecols\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39msentence\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mhindi_sentence\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mwords\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mhindi_words\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\dange\\.conda\\envs\\mlenv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dange\\.conda\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\dange\\.conda\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\dange\\.conda\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\dange\\.conda\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1219\u001b[0m     f,\n\u001b[0;32m   1220\u001b[0m     mode,\n\u001b[0;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1227\u001b[0m )\n\u001b[0;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\dange\\.conda\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    787\u001b[0m             handle,\n\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    792\u001b[0m         )\n\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../translation/translated_20.csv'"
     ]
    }
   ],
   "source": [
    "hindi_df = pd.read_csv('../translation/translated.csv', usecols=['sentence', 'hindi_sentence', 'words', 'hindi_words', 'index', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>index</th>\n",
       "      <th>words</th>\n",
       "      <th>type</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>hindi_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>luckily this kike has never lived under nazi r...</td>\n",
       "      <td>740</td>\n",
       "      <td>kike has never lived under nazi kikes sand nig...</td>\n",
       "      <td>train</td>\n",
       "      <td>सौभाग्य से यह Kike नाजी शासन के तहत कभी नहीं र...</td>\n",
       "      <td>Kike कभी भी नाजी किक सैंड निगर्स के तहत नहीं रहा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ohh man that is a fucking sad photo lots of re...</td>\n",
       "      <td>12989</td>\n",
       "      <td>sand nigger countries let the moslems kill eac...</td>\n",
       "      <td>train</td>\n",
       "      <td>ओह यार जो कि सैनिकों के लिए एक कमबख्त दुखद फोट...</td>\n",
       "      <td>रेत निगर देशों ने मोस्ले को एक दूसरे को मारने ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>miss lindsey is john mccains boy toy miss lind...</td>\n",
       "      <td>12636</td>\n",
       "      <td>demented faggot</td>\n",
       "      <td>train</td>\n",
       "      <td>मिस लिंडसे जॉन मैककेन्स बॉय टॉय मिस लिंडसे एक ...</td>\n",
       "      <td>डिम्फ़ फागोट</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asians dont deserve happiness</td>\n",
       "      <td>12054</td>\n",
       "      <td>asians dont deserve happiness</td>\n",
       "      <td>train</td>\n",
       "      <td>एशियाई लोग खुशी के लायक नहीं हैं</td>\n",
       "      <td>एशियाई लोग खुशी के लायक नहीं हैं</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; sikh community always helpful to others...</td>\n",
       "      <td>15220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;user&gt; सिख समुदाय हमेशा दूसरों के लिए सहायक रू...</td>\n",
       "      <td>#VALUE!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3842</th>\n",
       "      <td>&lt;user&gt; hari raya is not about people going fro...</td>\n",
       "      <td>1134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;उपयोगकर्ता&gt; हरि राया घरों से घरों तक जाने वाल...</td>\n",
       "      <td>#VALUE!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>no stop pointing the finger at men shame women...</td>\n",
       "      <td>1412</td>\n",
       "      <td>stop pointing the finger at men shame women wh...</td>\n",
       "      <td>test</td>\n",
       "      <td>पुरुषों पर उंगली की ओर इशारा करने वाला कोई भी ...</td>\n",
       "      <td>पुरुषों की उंगली की ओर इशारा करना बंद करो महिल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3844</th>\n",
       "      <td>america is so racist that they think asians ar...</td>\n",
       "      <td>386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>अमेरिका इतना नस्लवादी है कि उन्हें लगता है कि ...</td>\n",
       "      <td>#VALUE!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>this sperging alt right retard accused me of b...</td>\n",
       "      <td>293</td>\n",
       "      <td>this sperging alt right retard accused me of b...</td>\n",
       "      <td>test</td>\n",
       "      <td>इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...</td>\n",
       "      <td>इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>you promote articles by fat dyke pipe fitters ...</td>\n",
       "      <td>1100</td>\n",
       "      <td>by fat dyke pipe fitters</td>\n",
       "      <td>test</td>\n",
       "      <td>आप फैट डाइक पाइप फिटर द्वारा लेखों को बढ़ावा द...</td>\n",
       "      <td>#VALUE!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3847 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  index  \\\n",
       "0     luckily this kike has never lived under nazi r...    740   \n",
       "1     ohh man that is a fucking sad photo lots of re...  12989   \n",
       "2     miss lindsey is john mccains boy toy miss lind...  12636   \n",
       "3                         asians dont deserve happiness  12054   \n",
       "4     <user> sikh community always helpful to others...  15220   \n",
       "...                                                 ...    ...   \n",
       "3842  <user> hari raya is not about people going fro...   1134   \n",
       "3843  no stop pointing the finger at men shame women...   1412   \n",
       "3844  america is so racist that they think asians ar...    386   \n",
       "3845  this sperging alt right retard accused me of b...    293   \n",
       "3846  you promote articles by fat dyke pipe fitters ...   1100   \n",
       "\n",
       "                                                  words   type  \\\n",
       "0     kike has never lived under nazi kikes sand nig...  train   \n",
       "1     sand nigger countries let the moslems kill eac...  train   \n",
       "2                                       demented faggot  train   \n",
       "3                         asians dont deserve happiness  train   \n",
       "4                                                   NaN  train   \n",
       "...                                                 ...    ...   \n",
       "3842                                                NaN   test   \n",
       "3843  stop pointing the finger at men shame women wh...   test   \n",
       "3844                                                NaN   test   \n",
       "3845  this sperging alt right retard accused me of b...   test   \n",
       "3846                           by fat dyke pipe fitters   test   \n",
       "\n",
       "                                         hindi_sentence  \\\n",
       "0     सौभाग्य से यह Kike नाजी शासन के तहत कभी नहीं र...   \n",
       "1     ओह यार जो कि सैनिकों के लिए एक कमबख्त दुखद फोट...   \n",
       "2     मिस लिंडसे जॉन मैककेन्स बॉय टॉय मिस लिंडसे एक ...   \n",
       "3                      एशियाई लोग खुशी के लायक नहीं हैं   \n",
       "4     <user> सिख समुदाय हमेशा दूसरों के लिए सहायक रू...   \n",
       "...                                                 ...   \n",
       "3842  <उपयोगकर्ता> हरि राया घरों से घरों तक जाने वाल...   \n",
       "3843  पुरुषों पर उंगली की ओर इशारा करने वाला कोई भी ...   \n",
       "3844  अमेरिका इतना नस्लवादी है कि उन्हें लगता है कि ...   \n",
       "3845  इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...   \n",
       "3846  आप फैट डाइक पाइप फिटर द्वारा लेखों को बढ़ावा द...   \n",
       "\n",
       "                                            hindi_words  \n",
       "0      Kike कभी भी नाजी किक सैंड निगर्स के तहत नहीं रहा  \n",
       "1     रेत निगर देशों ने मोस्ले को एक दूसरे को मारने ...  \n",
       "2                                          डिम्फ़ फागोट  \n",
       "3                      एशियाई लोग खुशी के लायक नहीं हैं  \n",
       "4                                               #VALUE!  \n",
       "...                                                 ...  \n",
       "3842                                            #VALUE!  \n",
       "3843  पुरुषों की उंगली की ओर इशारा करना बंद करो महिल...  \n",
       "3844                                            #VALUE!  \n",
       "3845  इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...  \n",
       "3846                                            #VALUE!  \n",
       "\n",
       "[3847 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(hindi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_alphabets = [\n",
    "    \"क\",\"ख\",\"ग\",\"घ\",\"ङ\",\"च\",\"छ\",\"ज\",\"झ\",\"ञ\",\"ट\",\"ठ\",\"ड\",\"ढ\",\"ण\",\"त\",\"थ\",\"द\",\"ध\",\"न\",\"प\",\"फ\",\"ब\",\"भ\",\"म\",\"य\",\"र\",\"ल\",\"व\",\"श\",\"ष\",\"स\",\"ह\",\"क्ष\",\"त्र\",\"ज्ञ\"    \n",
    "    ]\n",
    "def normalize(input):\n",
    "    input_type = type(input)\n",
    "    if input_type == str:\n",
    "        input = input.split()\n",
    "    output = []\n",
    "    for word in input:\n",
    "        norm = []\n",
    "        for char in list(word):\n",
    "            if char in hindi_alphabets:\n",
    "                norm.append(char)\n",
    "        output.append(\"\".join(norm))\n",
    "    if input_type == str:\n",
    "        return \" \".join(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentence', 'index', 'words', 'type', 'hindi_sentence', 'hindi_words'])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_dict = hindi_df.to_dict()\n",
    "translated_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_sentences = hindi_df.hindi_sentence.values\n",
    "hindi_words = hindi_df.hindi_words.values\n",
    "english_words = hindi_df.words.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ae5eb19116432aadf769f21b298899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3847 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23065 words mapped out of 100875\n",
      "3681 rationales found out of 3847\n"
     ]
    }
   ],
   "source": [
    "hindi_rationales = []\n",
    "\n",
    "total_words = 0\n",
    "matches_found = 0\n",
    "sentences_matched = 0\n",
    "MATCHED = False\n",
    "for i, sentence in enumerate(tqdm(hindi_sentences)):\n",
    "    sentence = sentence.split()\n",
    "    rationale = torch.zeros(1, len(sentence))\n",
    "    total_words += len(hindi_words[i])\n",
    "    if hindi_words[i] == '#VALUE!':\n",
    "        hindi_rationales.append(rationale)\n",
    "        sentences_matched += 1\n",
    "        continue\n",
    "    transliterated_words = []\n",
    "    if type(hindi_words[i]) != list: \n",
    "        hindi_words[i] = hindi_words[i].split() \n",
    "    if type(english_words[i]) != list: \n",
    "        english_words[i] = english_words[i].split()\n",
    "        for word in english_words[i]:\n",
    "            transliterated_words.extend(transliterate_word(word, lang_code='hi'))\n",
    "    normalized_hindi_words = normalize(hindi_words[i])\n",
    "    normalized_trans_words = normalize(transliterated_words)\n",
    "    for j, word in enumerate(sentence):\n",
    "        if word in hindi_words[i] or word in english_words[i] or word in transliterated_words:\n",
    "            matches_found += 1\n",
    "            rationale[0,j] = 1\n",
    "            if not MATCHED:\n",
    "                sentences_matched += 1\n",
    "                MATCHED = True\n",
    "        elif normalize(word) in normalized_hindi_words or normalize(word) in normalized_trans_words:\n",
    "            matches_found += 1\n",
    "            rationale[0, j] = 1\n",
    "            if not MATCHED:\n",
    "                sentences_matched += 1\n",
    "                MATCHED = True\n",
    "    if not MATCHED:\n",
    "        hindi_rationales.append(None)\n",
    "    else:\n",
    "        hindi_rationales.append(rationale)\n",
    "    MATCHED = False\n",
    "print(f'{matches_found} words mapped out of {total_words}\\n{sentences_matched} rationales found out of {hindi_df.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dange\\.conda\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:576: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "c:\\Users\\dange\\.conda\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:576: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>index</th>\n",
       "      <th>words</th>\n",
       "      <th>type</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>hindi_words</th>\n",
       "      <th>hindi_rationales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>luckily this kike has never lived under nazi r...</td>\n",
       "      <td>740</td>\n",
       "      <td>kike has never lived under nazi kikes sand nig...</td>\n",
       "      <td>train</td>\n",
       "      <td>सौभाग्य से यह Kike नाजी शासन के तहत कभी नहीं र...</td>\n",
       "      <td>Kike कभी भी नाजी किक सैंड निगर्स के तहत नहीं रहा</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ohh man that is a fucking sad photo lots of re...</td>\n",
       "      <td>12989</td>\n",
       "      <td>sand nigger countries let the moslems kill eac...</td>\n",
       "      <td>train</td>\n",
       "      <td>ओह यार जो कि सैनिकों के लिए एक कमबख्त दुखद फोट...</td>\n",
       "      <td>रेत निगर देशों ने मोस्ले को एक दूसरे को मारने ...</td>\n",
       "      <td>[[tensor(1.), tensor(0.), tensor(1.), tensor(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>miss lindsey is john mccains boy toy miss lind...</td>\n",
       "      <td>12636</td>\n",
       "      <td>demented faggot</td>\n",
       "      <td>train</td>\n",
       "      <td>मिस लिंडसे जॉन मैककेन्स बॉय टॉय मिस लिंडसे एक ...</td>\n",
       "      <td>डिम्फ़ फागोट</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asians dont deserve happiness</td>\n",
       "      <td>12054</td>\n",
       "      <td>asians dont deserve happiness</td>\n",
       "      <td>train</td>\n",
       "      <td>एशियाई लोग खुशी के लायक नहीं हैं</td>\n",
       "      <td>एशियाई लोग खुशी के लायक नहीं हैं</td>\n",
       "      <td>[[tensor(1.), tensor(1.), tensor(1.), tensor(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; sikh community always helpful to others...</td>\n",
       "      <td>15220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;user&gt; सिख समुदाय हमेशा दूसरों के लिए सहायक रू...</td>\n",
       "      <td>#VALUE!</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3842</th>\n",
       "      <td>&lt;user&gt; hari raya is not about people going fro...</td>\n",
       "      <td>1134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;उपयोगकर्ता&gt; हरि राया घरों से घरों तक जाने वाल...</td>\n",
       "      <td>#VALUE!</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>no stop pointing the finger at men shame women...</td>\n",
       "      <td>1412</td>\n",
       "      <td>stop pointing the finger at men shame women wh...</td>\n",
       "      <td>test</td>\n",
       "      <td>पुरुषों पर उंगली की ओर इशारा करने वाला कोई भी ...</td>\n",
       "      <td>पुरुषों की उंगली की ओर इशारा करना बंद करो महिल...</td>\n",
       "      <td>[[tensor(1.), tensor(0.), tensor(1.), tensor(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3844</th>\n",
       "      <td>america is so racist that they think asians ar...</td>\n",
       "      <td>386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>अमेरिका इतना नस्लवादी है कि उन्हें लगता है कि ...</td>\n",
       "      <td>#VALUE!</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>this sperging alt right retard accused me of b...</td>\n",
       "      <td>293</td>\n",
       "      <td>this sperging alt right retard accused me of b...</td>\n",
       "      <td>test</td>\n",
       "      <td>इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...</td>\n",
       "      <td>इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...</td>\n",
       "      <td>[[tensor(1.), tensor(1.), tensor(1.), tensor(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>you promote articles by fat dyke pipe fitters ...</td>\n",
       "      <td>1100</td>\n",
       "      <td>by fat dyke pipe fitters</td>\n",
       "      <td>test</td>\n",
       "      <td>आप फैट डाइक पाइप फिटर द्वारा लेखों को बढ़ावा द...</td>\n",
       "      <td>#VALUE!</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3847 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  index  \\\n",
       "0     luckily this kike has never lived under nazi r...    740   \n",
       "1     ohh man that is a fucking sad photo lots of re...  12989   \n",
       "2     miss lindsey is john mccains boy toy miss lind...  12636   \n",
       "3                         asians dont deserve happiness  12054   \n",
       "4     <user> sikh community always helpful to others...  15220   \n",
       "...                                                 ...    ...   \n",
       "3842  <user> hari raya is not about people going fro...   1134   \n",
       "3843  no stop pointing the finger at men shame women...   1412   \n",
       "3844  america is so racist that they think asians ar...    386   \n",
       "3845  this sperging alt right retard accused me of b...    293   \n",
       "3846  you promote articles by fat dyke pipe fitters ...   1100   \n",
       "\n",
       "                                                  words   type  \\\n",
       "0     kike has never lived under nazi kikes sand nig...  train   \n",
       "1     sand nigger countries let the moslems kill eac...  train   \n",
       "2                                       demented faggot  train   \n",
       "3                         asians dont deserve happiness  train   \n",
       "4                                                   NaN  train   \n",
       "...                                                 ...    ...   \n",
       "3842                                                NaN   test   \n",
       "3843  stop pointing the finger at men shame women wh...   test   \n",
       "3844                                                NaN   test   \n",
       "3845  this sperging alt right retard accused me of b...   test   \n",
       "3846                           by fat dyke pipe fitters   test   \n",
       "\n",
       "                                         hindi_sentence  \\\n",
       "0     सौभाग्य से यह Kike नाजी शासन के तहत कभी नहीं र...   \n",
       "1     ओह यार जो कि सैनिकों के लिए एक कमबख्त दुखद फोट...   \n",
       "2     मिस लिंडसे जॉन मैककेन्स बॉय टॉय मिस लिंडसे एक ...   \n",
       "3                      एशियाई लोग खुशी के लायक नहीं हैं   \n",
       "4     <user> सिख समुदाय हमेशा दूसरों के लिए सहायक रू...   \n",
       "...                                                 ...   \n",
       "3842  <उपयोगकर्ता> हरि राया घरों से घरों तक जाने वाल...   \n",
       "3843  पुरुषों पर उंगली की ओर इशारा करने वाला कोई भी ...   \n",
       "3844  अमेरिका इतना नस्लवादी है कि उन्हें लगता है कि ...   \n",
       "3845  इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...   \n",
       "3846  आप फैट डाइक पाइप फिटर द्वारा लेखों को बढ़ावा द...   \n",
       "\n",
       "                                            hindi_words  \\\n",
       "0      Kike कभी भी नाजी किक सैंड निगर्स के तहत नहीं रहा   \n",
       "1     रेत निगर देशों ने मोस्ले को एक दूसरे को मारने ...   \n",
       "2                                          डिम्फ़ फागोट   \n",
       "3                      एशियाई लोग खुशी के लायक नहीं हैं   \n",
       "4                                               #VALUE!   \n",
       "...                                                 ...   \n",
       "3842                                            #VALUE!   \n",
       "3843  पुरुषों की उंगली की ओर इशारा करना बंद करो महिल...   \n",
       "3844                                            #VALUE!   \n",
       "3845  इस स्पर्गिंग ऑल राइट रिटार्ड ने मुझ पर एक विध्...   \n",
       "3846                                            #VALUE!   \n",
       "\n",
       "                                       hindi_rationales  \n",
       "0     [[tensor(0.), tensor(0.), tensor(0.), tensor(1...  \n",
       "1     [[tensor(1.), tensor(0.), tensor(1.), tensor(1...  \n",
       "2     [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  \n",
       "3     [[tensor(1.), tensor(1.), tensor(1.), tensor(1...  \n",
       "4     [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  \n",
       "...                                                 ...  \n",
       "3842  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  \n",
       "3843  [[tensor(1.), tensor(0.), tensor(1.), tensor(1...  \n",
       "3844  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  \n",
       "3845  [[tensor(1.), tensor(1.), tensor(1.), tensor(1...  \n",
       "3846  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  \n",
       "\n",
       "[3847 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'hindi_rationales' in hindi_df.columns:\n",
    "    print(\"cleaning . . . \")\n",
    "    hindi_df.drop(columns=['hindi_rationales'])\n",
    "    hindi_df.dropna()\n",
    "hindi_df = pd.concat([hindi_df, pd.DataFrame(hindi_rationales, columns=['hindi_rationales'])], axis=1)\n",
    "display(hindi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_english(en_dict, index, label, rationale, output, post_tokens):\n",
    "    en_dict['index'] = np.concatenate((en_dict['index'], np.array([index])))\n",
    "    en_dict['label'] = np.concatenate((en_dict['label'], label.reshape(1,3)))\n",
    "    en_dict['rationales'] = np.concatenate((en_dict['rationales'], rationale.reshape(1, 200)))\n",
    "    en_dict['class'] = np.concatenate((en_dict['class'], np.array([output])))\n",
    "    en_dict['post_tokens'].append(post_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'label', 'rationales', 'class', 'post_tokens'])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final(translated_dict, en_dict, hi_dict, hindi_df):\n",
    "    for i, row in enumerate(hindi_df.iterrows()):\n",
    "        _, row = row\n",
    "        if row[-1] == None:\n",
    "            append_to_english(en_dict, row[1], hi_dict['label'][i], hi_dict['rationales'][i], hi_dict['class'][i], hi_dict['post_tokens'][i])\n",
    "        else:\n",
    "            translated_dict['index'].append(row[1])\n",
    "            translated_dict['label'].append(hi_dict['label'][i])\n",
    "            translated_dict['rationales'].append(row[-1])\n",
    "            translated_dict['class'].append(hi_dict['class'][i])\n",
    "            translated_dict['post_tokens'].append(row[4].split(\" \"))\n",
    "    translated_dict['label'] = torch.stack(translated_dict['label'])\n",
    "    translated_dict['class'] = torch.stack(translated_dict['class'])\n",
    "    rationales = []\n",
    "    for rationale in translated_dict['rationales']:\n",
    "        r = np.concatenate(\n",
    "            (rationale, \n",
    "                np.zeros((1, 200 - rationale.shape[1]))\n",
    "            ), axis=1).astype(bool)\n",
    "        rationales.append(torch.tensor((r).astype(int)))\n",
    "    translated_dict['rationales'] = torch.stack(rationales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_hi_train = {}\n",
    "translated_hi_validation = {}\n",
    "translated_hi_test = {}\n",
    "\n",
    "for key in en_train.keys():\n",
    "    translated_hi_train[key] = []\n",
    "    translated_hi_validation[key] = []\n",
    "    translated_hi_test[key] = []\n",
    "\n",
    "groups = hindi_df.groupby('type')\n",
    "generate_final(translated_hi_train, en_train, hi_train, groups.get_group('train'))\n",
    "generate_final(translated_hi_validation, en_validation, hi_validation, groups.get_group('validation'))\n",
    "generate_final(translated_hi_test, en_test, hi_test, groups.get_group('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train class split:-  tensor([ 879, 1250,  816]) \n",
      " validation class split:-  tensor([114, 156, 104]) \n",
      " test class split:-  tensor([110, 156,  96])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"train class split:- \", translated_hi_train['class'].bincount(), \"\\n\",\n",
    "    \"validation class split:- \", translated_hi_validation['class'].bincount(), \"\\n\",\n",
    "    \"test class split:- \", translated_hi_test['class'].bincount(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train class split:-  tensor([3869, 5001, 3568]) \n",
      " validation class split:-  tensor([479, 625, 444]) \n",
      " test class split:-  tensor([484, 626, 452])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"train class split:- \", torch.tensor(en_train['class']).bincount(), \"\\n\",\n",
    "    \"validation class split:- \", torch.tensor(en_validation['class']).bincount(), \"\\n\",\n",
    "    \"test class split:- \", torch.tensor(en_test['class']).bincount(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4748, 6251, 4384]) tensor([593, 781, 548]) tensor([594, 782, 548])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    torch.tensor(en_train['class']).bincount() + translated_hi_train['class'].bincount(), \n",
    "torch.tensor(en_validation['class']).bincount() + translated_hi_validation['class'].bincount(), \n",
    "torch.tensor(en_test['class']).bincount() + translated_hi_test['class'].bincount()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train class split:-  tensor([4748, 6251, 4384]) \n",
      " validation class split:-  tensor([593, 781, 548]) \n",
      " test class split:-  tensor([594, 782, 548])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"train class split:- \", train['class'].bincount(), \"\\n\",\n",
    "    \"validation class split:- \", validation['class'].bincount(), \"\\n\",\n",
    "    \"test class split:- \", test['class'].bincount(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(en_train, open('../data/en_train.p', 'wb'))\n",
    "pickle.dump(en_validation, open('../data/en_validation.p', 'wb'))\n",
    "pickle.dump(en_test, open('../data/en_test.p', 'wb'))\n",
    "\n",
    "pickle.dump(translated_hi_train, open('../data/hi_train.p', 'wb'))\n",
    "pickle.dump(hi_validation, open('../data/hi_validation.p', 'wb'))\n",
    "pickle.dump(hi_test, open('../data/hi_test.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c165b169ead51ed7dd867ada967038e7afce51eef97009d1ebd4bca797cfdb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
